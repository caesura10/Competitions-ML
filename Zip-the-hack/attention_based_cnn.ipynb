{"cells":[{"metadata":{"trusted":true,"_uuid":"64688166e6bd7f178a601fcd56e1f47d89586204"},"cell_type":"code","source":"import  numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5518a64074efdee763cac0ca838b835b3fcda3e9"},"cell_type":"code","source":"import string\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcbadce7f294756a96cb4b9d8d8b859104225ffb"},"cell_type":"code","source":"from nltk.util import ngrams\nfrom collections import Counter\nfrom gensim.models import word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c296a77f6d2590569471aef0f19038e99cf4382"},"cell_type":"code","source":"data = pd.read_csv('../input/ziphack/train_file.csv')\ntest = pd.read_csv('../input/ziphack/test_file.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9326f85292e0e0b809f5376de188a844e786734"},"cell_type":"code","source":"data[\"MaterialType\"] = data['MaterialType'].str.replace('[^\\w\\s]','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cf6c267e047cb90b59542362b38111932982011"},"cell_type":"code","source":"data.drop(['UsageClass','CheckoutType','CheckoutYear','CheckoutMonth','ID'],inplace=True,axis=1)\ntest.drop(['UsageClass','CheckoutType','CheckoutYear','CheckoutMonth'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f374459e639c6ce49f1403f8e52aa9c03ae3520"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing as skp\nlabel_encoder = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6f0e775fc9ea19ae3b942445eaf4312d85b8d89"},"cell_type":"code","source":"led = LabelEncoder()\nled.fit(data[\"MaterialType\"])\n\ndata[\"MaterialType\"] = led.transform(data[\"MaterialType\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d056edd78534afd26cbeabc455cfcf6d61f4c1fc"},"cell_type":"code","source":"y = data[\"MaterialType\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac438f67928e13e8314c0d3bf85c69912c5f8ea5"},"cell_type":"code","source":"data = data.apply(lambda x: x.astype(str).str.lower())\ntest = test.apply(lambda x: x.astype(str).str.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b3a5b7c2ab0b0e6f43deb3887056b0e73fdbce6"},"cell_type":"code","source":"data[\"Title\"] = data['Title'].str.replace('[^\\w\\s]','')\ntest[\"Title\"] = test['Title'].str.replace('[^\\w\\s]','')\ndata[\"Subjects\"] = data['Subjects'].str.replace('[^\\w\\s]','')\ntest[\"Subjects\"] = test['Subjects'].str.replace('[^\\w\\s]','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0504c92e97e2e46580a5543c0084b404bec5140d"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bce113cf89e584593aca95d1a99358d7b365f51"},"cell_type":"code","source":"from nltk.corpus import stopwords\nsw = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac795358ee7ff7a0caf973f4080509a1e4996d30"},"cell_type":"code","source":"def stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    return \" \".join(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d0418103106222b42629c8f88e561c805b6450b"},"cell_type":"code","source":"data['Title'] = data['Title'].apply(stopwords)\ndata['Subjects'] = data['Subjects'].apply(stopwords)\ntest['Subjects'] = test['Subjects'].apply(stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"879b9f9520f56a94f53c41ee4a3d980c716c3834"},"cell_type":"code","source":"import nltk\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\nstemmer = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aec2a9a74995c22744d60a1cfd2a7cad743bae1f"},"cell_type":"code","source":"def pre(data):\n    word = lemmatizer.lemmatize(data) \n    return word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b3453827555163e62366871ef48e153a03d2f3"},"cell_type":"code","source":"data['Title'] = data['Title'].apply(pre)\ndata['Subjects'] = data['Subjects'].apply(pre)\ntest['Subjects'] = test['Subjects'].apply(pre)\ntest['Title'] = test['Title'].apply(pre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b49641c092ce281b21865ec9f6af61462ba788e2"},"cell_type":"code","source":"data[\"Subjects\"] = data['Subjects'].replace('nan','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c148d0d88fde3d8a406de9981d76bc2e855ded1"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35a99927a02fcc5fdf08f1b54f97adf1575cef9a"},"cell_type":"code","source":"data[\"Phrase\"] = data[\"Title\"].map(str) + ' ' + data[\"Subjects\"] \ntest[\"Phrase\"] = test[\"Title\"].map(str) + ' ' + test[\"Subjects\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed76acf65e798bc14477dbfc5a6efc102370c57a"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1fca1b8d2acdbee9b08a1aeb96a38ec2bd83e24"},"cell_type":"code","source":"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras.layers import *\nfrom keras.models import Model\n\n\n# For reproducibility.\nseed = 7\nnp.random.seed(seed)\ntf.set_random_seed(seed)\nsession_conf = tf.ConfigProto(\n    intra_op_parallelism_threads=1,\n    inter_op_parallelism_threads=1\n)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8714274c666e628f8228b1fae7307a2998fc66aa"},"cell_type":"code","source":"data.Phrase = data.Phrase.apply(lambda x: re.sub(r'[0-9]+', '0', x))\ntest.Phrase = test.Phrase.apply(lambda x: re.sub(r'[0-9]+', '0', x))\n\nx_train = data['Phrase'].values\nx_test  = test['Phrase'].values\ny_train = data['MaterialType'].values\nx = np.r_[x_train, x_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dee8735eed72b44d7d57da1e3a6e8777f9edfa7"},"cell_type":"code","source":"tokenizer = Tokenizer(lower=True, filters='\\n\\t')\ntokenizer.fit_on_texts(x)\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test  = tokenizer.texts_to_sequences(x_test)\nvocab_size = len(tokenizer.word_index) + 1  # +1 is for zero padding.\nprint('vocabulary size: {}'.format(vocab_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7913c42a3d3be256b75a4ebbcc74e88eb160986d"},"cell_type":"code","source":"EMBEDDING_FILE =  '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n\ndef load_embeddings(filename):\n    embeddings = {}\n    with open(filename) as f:\n        for line in f:\n            values = line.rstrip().split(' ')\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n\nembeddings = load_embeddings(EMBEDDING_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cff4210f56028cec96e6c0f455bb1406b2983cc"},"cell_type":"code","source":"maxlen = len(max((s for s in np.r_[x_train, x_test]), key=len))\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen, padding='post')\nprint('maxlen: {}'.format(maxlen))\nprint(x_train.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2a4d089b59f75260a4bd656cc55285102e8c95c"},"cell_type":"code","source":"def filter_embeddings(embeddings, word_index, vocab_size, dim=300):\n    embedding_matrix = np.zeros([vocab_size, dim])\n    for word, i in word_index.items():\n        if i >= vocab_size:\n            continue\n        vector = embeddings.get(word)\n        if vector is not None:\n            embedding_matrix[i] = vector\n    return embedding_matrix\n\nembedding_size = 300\nembedding_matrix = filter_embeddings(embeddings, tokenizer.word_index,\n                                     vocab_size, embedding_size)\nprint('OOV: {}'.format(len(set(tokenizer.word_index) - set(embeddings))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffef445ab11b6687b7f965657778c60f1ef0308f"},"cell_type":"code","source":"class Attention(Layer):\n    \"\"\"\n    Keras Layer that implements an Attention mechanism for temporal data.\n    Supports Masking.\n    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    :param kwargs:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(Attention())\n    \"\"\"\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0e08f78c4724effb1e01098f7a2dc07c7ea32cb"},"cell_type":"code","source":"def build_model(maxlen, vocab_size, embedding_size, embedding_matrix):\n    input_words = Input((maxlen, ))\n    x_words = Embedding(vocab_size,\n                        embedding_size,\n                        weights=[embedding_matrix],\n                        mask_zero=True,\n                        trainable=False)(input_words)\n    x_words = SpatialDropout1D(0.3)(x_words)\n    x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n    x = Attention(maxlen)(x_words)\n    x = Dropout(0.2)(x)\n    x = Dense(50, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    pred = Dense(8, activation='softmax')(x)\n\n    model = Model(inputs=input_words, outputs=pred)\n    return model\n\nmodel = build_model(maxlen, vocab_size, embedding_size, embedding_matrix)\nmodel.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"574488f269ce08e44dabc8a69d86c341bdb88e27"},"cell_type":"code","source":"save_file = 'model.h5'\nhistory = model.fit(x_train, y_train,\n                    epochs=20, verbose=1,\n                    batch_size=1024, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fef730dd38ffc1be0c77e27fd2d2b0523ff0055f"},"cell_type":"code","source":"y_pred = model.predict(x_test, batch_size=1024)\ny_pred = y_pred.argmax(axis=1).astype(int)\ny_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3492a2eff941f49ba7f417b8f414d775482ba6a"},"cell_type":"code","source":"submission = pd.DataFrame({'ID':test['ID'],'MaterialType': y_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6ff993a82c161943bb5d5636057584ab01bce2c"},"cell_type":"code","source":"submission[\"MaterialType\"] = led.inverse_transform(submission[\"MaterialType\"])\nsubmission = submission.apply(lambda x: x.astype(str).str.upper())\nsubmission.to_csv('submission_cnn.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"464154894ce0e3c32756ebc04375348bf4e40874"},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09324542a7496797e3e287b00958ba1ef4e7fdf2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}