{"cells":[{"metadata":{"trusted":true,"_uuid":"a223e76b9b8826aac5c860f14703ba330f494020"},"cell_type":"code","source":"import  numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9573709d5cdf6690fc4c86b59377a31fe17d524e"},"cell_type":"code","source":"import string\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79008e2dfdbaf455f5f579ad4fbc9d0e151b3c8f"},"cell_type":"code","source":"import statsmodels.api as sm\nfrom collections import Counter\nfrom gensim.models import word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b6054ccd8656ab6abe87775363658fc98037164"},"cell_type":"code","source":"data = pd.read_csv('../input/ziphack/train_file.csv')\ntest = pd.read_csv('../input/ziphack/test_file.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa8ab4b5e50cff4c631a0e3df0e73e9ebacefd47"},"cell_type":"code","source":"data[\"MaterialType\"] = data['MaterialType'].str.replace('[^\\w\\s]','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ab5278a91fed7a1ee1d7445d80cb022bcc61e85"},"cell_type":"code","source":"data.drop(['UsageClass','CheckoutType','CheckoutYear','CheckoutMonth'],inplace=True,axis=1)\ntest.drop(['UsageClass','CheckoutType','CheckoutYear','CheckoutMonth'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8116fb845db51f2998bfb488781dad83361b43e7"},"cell_type":"code","source":"data.drop(['Checkouts','PublicationYear'],inplace = True,axis = 1)\ntest.drop(['Checkouts','PublicationYear'],inplace = True,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8410c26d14b235688b7b7e68f8a3ed297f741f46"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"598d2a7b39e1d7f1b40707070f08a164e5b6cf5c"},"cell_type":"code","source":"y = data[\"MaterialType\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdb2f2bfd6c799a16bc62aba208a53f9b722382c"},"cell_type":"code","source":"data = data.apply(lambda x: x.astype(str).str.lower())\ntest = test.apply(lambda x: x.astype(str).str.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96b5f4667fb6e0bf444b5db9b34ed6ee5fedd71a"},"cell_type":"code","source":"data[\"Title\"] = data['Title'].str.replace('[^\\w\\s]','')\ntest[\"Title\"] = test['Title'].str.replace('[^\\w\\s]','')\ndata[\"Subjects\"] = data['Subjects'].str.replace('[^\\w\\s]','')\ntest[\"Subjects\"] = test['Subjects'].str.replace('[^\\w\\s]','')\n##test[\"Subjects\"] = test['Subjects'].str.replace('[0-9]','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a258bfb4ec8e48a64dc3651814ae2629b4014f5b"},"cell_type":"code","source":"from nltk.corpus import stopwords\nsw = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc0a2ad96d5a70881d6bd2716c25421cbd0e64f7"},"cell_type":"code","source":"sw = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21fcdfa25ed4a5b15b16401dbabd34d303247085"},"cell_type":"code","source":"def stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    return \" \".join(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84d6215324a429fe519129d9b8e0a48719fc9b4d"},"cell_type":"code","source":"data['Title'] = data['Title'].apply(stopwords)\ndata['Subjects'] = data['Subjects'].apply(stopwords)\ntest['Title'] = test['Title'].apply(stopwords)\ntest['Subjects'] = test['Subjects'].apply(stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2019b735c3fc719f6829517167b69182cb3ccf17"},"cell_type":"code","source":"#print(data.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2bb7250590ffcd93665ba0a479ea138e188c9bc"},"cell_type":"code","source":"import nltk\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\nstemmer = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc13d62f62406644000b6fa786e6a13edc70908d"},"cell_type":"code","source":"def pre(data):\n    word = lemmatizer.lemmatize(data) \n    return word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9ecbaadb8c7659de4e6e96ec77af3df76d554ad"},"cell_type":"code","source":"data['Title'] = data['Title'].apply(pre)\ndata['Subjects'] = data['Subjects'].apply(pre)\ntest['Subjects'] = test['Subjects'].apply(pre)\ntest['Title'] = test['Title'].apply(pre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5c024c0e65d2d0089283d2a746ede0385df1d37"},"cell_type":"code","source":"data[\"Subjects\"] = data['Subjects'].replace('nan','')\ntest[\"Subjects\"] = test['Subjects'].replace('nan','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c5075ca41067b517e82d8241d2d0f3f93ae25fc"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c5a8eed7c01b0f9bdce2fa74dd5d908fe575ab6"},"cell_type":"code","source":"data[\"Final\"] = data[\"Title\"].map(str) + ' ' + data[\"Subjects\"]\ntest[\"Final\"] = test[\"Title\"].map(str) + ' ' + test[\"Subjects\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74b56a5ad4e59257521b2c2a854a39210eb40fbf"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing as skp\nlabel_encoder = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82757a04724798b0029e7a4b308cc29fcff1dbe7"},"cell_type":"code","source":"led = LabelEncoder()\nled.fit(data[\"MaterialType\"])\ndata[\"MaterialType\"] = led.transform(data[\"MaterialType\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f01ab2a8ef796af6e4cd69ecf40ac2dabd9b849"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nfrom keras import backend\nimport logging","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"768016adf4106457c9f2f346897321c518607e1a"},"cell_type":"code","source":"from keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\nvocab_size = 20000  # based on words in the entire corpus\nmax_len = 90        # based on word count in phrases\n\nall_corpus   = list(data['Final'].values) + list(test['Final'].values)\ntrain_phrases  = list(data['Final'].values) \ntest_phrases   = list(test['Final'].values)\nX_train_target_binary = pd.get_dummies(data['MaterialType'])\n\n#Vocabulary-Indexing of thetrain and test phrases, make sure \"filters\" parm doesn't clean out punctuations\n\ntokenizer = Tokenizer(num_words=vocab_size, lower=True, filters='\\n\\t')\ntokenizer.fit_on_texts(all_corpus)\nencoded_train_phrases = tokenizer.texts_to_sequences(train_phrases)\nencoded_test_phrases = tokenizer.texts_to_sequences(test_phrases)\n\n#Watch for a POST padding, as opposed to the default PRE padding\n\nX_train_words = sequence.pad_sequences(encoded_train_phrases, maxlen=max_len,  padding='post')\nX_test_words = sequence.pad_sequences(encoded_test_phrases, maxlen=max_len,  padding='post')\nprint (X_train_words.shape)\nprint (X_test_words.shape)\nprint (X_train_target_binary.shape)\n\nprint ('Done Tokenizing and indexing phrases based on the vocabulary learned from the entire Train and Test corpus')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f08d1fd5b9d00a77ecf9ab9a5bacf436ad01a600"},"cell_type":"code","source":"data.groupby('MaterialType')['ID'].agg('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"065461dd7f9e3ad80381562b612bfd572e355b2b"},"cell_type":"code","source":"save_test = pd.merge(test, data[[\"Final\", \"MaterialType\"]], on=\"Final\", how=\"inner\")\nprint (\"Number of overlapping phrases  \", save_test.shape[0])\n#print (\"% of neutral sentiment phrases\",save_test[(save_test['Sentiment'] == 2)].count()[0] /save_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"118546359d9c200b0689d59ffea23fd12b81c6cb"},"cell_type":"code","source":"word_index = tokenizer.word_index\nembeddings_index = {}\nembedding_size = 300\nEMBEDDING_FILE =  '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n\nwith open(EMBEDDING_FILE, 'r') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n        \nnum_words = min(vocab_size, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_size))\nfor word, i in word_index.items():\n    if i >= vocab_size:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        \nlogging.info('Done building embedding matrix from FastText') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad68fc1d11c6e6771ed56b8424048c5329149224"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import  GlobalMaxPool1D, SpatialDropout1D\nfrom keras.layers import Bidirectional\nfrom keras.models import Model\nfrom keras import optimizers\n\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode=\"min\", patience = 3, verbose=1)\n\nprint(\"Building layers\")        \nnb_epoch = 25\nprint('starting to stitch and compile  model')\n\n# Embedding layer for text inputs\ninput_words = Input((max_len, ))\nx_words = Embedding(num_words, embedding_size,weights=[embedding_matrix],trainable=False)(input_words)\nx_words = SpatialDropout1D(0.2)(x_words)\nx_words = Bidirectional(CuDNNGRU(50, return_sequences=True))(x_words)\nx_words = Dropout(0.2)(x_words)\nx_words = Conv1D(128, 1, strides = 1,  padding='causal', activation='relu', )(x_words)\nx_words = Conv1D(256, 3, strides = 1,  padding='causal', activation='relu', )(x_words)\nx_words = Conv1D(512, 5, strides = 1,   padding='causal', activation='relu', )(x_words)\nx_words = GlobalMaxPool1D()(x_words)\nx_words = Dropout(0.2)(x_words)\n\nx = Dense(50, activation=\"relu\")(x_words)\nx = Dropout(0.2)(x)\npredictions = Dense(8, activation=\"softmax\")(x)\n\nmodel = Model(inputs=[input_words], outputs=predictions)\nmodel.compile(optimizer='rmsprop' ,loss='categorical_crossentropy', metrics=['accuracy'])\nprint(model.summary())\n\n\nprint(\"OOV word count:\", len(set(word_index) - set(embeddings_index)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"132a38dc182051b42e1ff50259ac736c7915fead"},"cell_type":"code","source":"history = model.fit([X_train_words], X_train_target_binary, epochs=nb_epoch, verbose=1, batch_size = 512, callbacks=[early_stop], validation_split = 0.05, shuffle=True)\n#history = model.fit(X_train_words, X_train_target_binary, epochs=10, verbose=1, batch_size = 512,  validation_split = 0.1, shuffle=True)\ntrain_loss = np.mean(history.history['loss'])\nval_loss = np.mean(history.history['val_loss'])\nprint('Train loss: %f' % (train_loss*100))\nprint('Validation loss: %f' % (val_loss*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ffb436e817c1b1df574f5862e96a38bd2625a7d"},"cell_type":"code","source":"pred_test = model.predict([X_test_words], batch_size=1024, verbose = 0)\nprint (pred_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3c9e472c59b781cc9bc309fc3d6fdc55ab2acc8"},"cell_type":"code","source":"max_pred = np.round(np.argmax(pred_test, axis=1)).astype(int)\nsubmission = pd.DataFrame({'ID':test['ID'],'MaterialType': max_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32be7caa567d30181ea2496a0481030bc2384955"},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"596b68a585f918fd3d82340f612e5bc350d243d8"},"cell_type":"code","source":"submission[\"MaterialType\"] = led.inverse_transform(submission[\"MaterialType\"])\nsubmission = submission.apply(lambda x: x.astype(str).str.upper())\nsubmission.to_csv('submission_cnn.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"787cef60869dbd3bc74d3a8205b52ffb0178e828"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}